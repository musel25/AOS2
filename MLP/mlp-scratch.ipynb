{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e432bfdd",
   "metadata": {},
   "source": [
    "Demonstration: stacking multiple **linear** layers (no activations) \n",
    "is exactly equivalent to a single affine (linear + bias) transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c2b6e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e7f9f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "442790c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions\n",
    "n_samples = 8      # batch size\n",
    "d_in = 5           # input dimension\n",
    "hidden = [7, 6, 4] # three \"hidden\" linear layers (no activations)\n",
    "d_out = 3          # output dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90ae8df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dummy input data\n",
    "X = rng.normal(size=(n_samples, d_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2fff4ff",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Build random linear layers (weights and biases)\n",
    "def random_layer(in_dim, out_dim, scale=0.5):\n",
    "    W = rng.normal(scale=scale, size=(in_dim, out_dim))\n",
    "    b = rng.normal(scale=scale, size=(out_dim,))\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1caa652a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "dims = [d_in] + hidden + [d_out]\n",
    "Ws, bs = [], []\n",
    "for i in range(len(dims)-1):\n",
    "    W, b = random_layer(dims[i], dims[i+1])\n",
    "    Ws.append(W)\n",
    "    bs.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee20a3a2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Forward pass through the stack of linear layers (no activations)\n",
    "def forward_stack(X, Ws, bs):\n",
    "    H = X\n",
    "    for W, b in zip(Ws, bs):\n",
    "        H = H @ W + b  # affine map\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "000fa916",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "Y_stack = forward_stack(X, Ws, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4dfe1a5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Collapse the stack into a single affine map: Y = X @ W_eff + b_eff\n",
    "def collapse_affine(Ws, bs):\n",
    "    W_eff = Ws[0].copy()\n",
    "    b_eff = bs[0].copy()\n",
    "    for W, b in zip(Ws[1:], bs[1:]):\n",
    "        # After previous layers we have: H = X @ W_eff + b_eff\n",
    "        # Next layer outputs: H @ W + b = (X @ W_eff + b_eff) @ W + b\n",
    "        #                   = X @ (W_eff @ W) + (b_eff @ W + b)\n",
    "        b_eff = b_eff @ W + b\n",
    "        W_eff = W_eff @ W\n",
    "    return W_eff, b_eff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "daae42a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_eff, b_eff = collapse_affine(Ws, bs)\n",
    "Y_eff = X @ W_eff + b_eff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39d3106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results\n",
    "max_abs_diff = np.max(np.abs(Y_stack - Y_eff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f8abdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "  X:      (8, 5)\n",
      "  Layer 1  W: (5, 7),  b: (7,)\n",
      "  Layer 2  W: (7, 6),  b: (6,)\n",
      "  Layer 3  W: (6, 4),  b: (4,)\n",
      "  Layer 4  W: (4, 3),  b: (3,)\n",
      "  Collapsed W_eff: (5, 3), b_eff: (3,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Shapes:\")\n",
    "print(f\"  X:      {X.shape}\")\n",
    "for i, (W, b) in enumerate(zip(Ws, bs), 1):\n",
    "    print(f\"  Layer {i}  W: {W.shape},  b: {b.shape}\")\n",
    "print(f\"  Collapsed W_eff: {W_eff.shape}, b_eff: {b_eff.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11f83bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 rows of Y from stacked linear layers:\n",
      " [[-1.69690669  2.33362122 -2.61461271]\n",
      " [-1.26940596  1.42219875 -2.00695106]\n",
      " [-0.79445137  1.42375524 -1.06462945]]\n",
      "\n",
      "First 3 rows of Y from single affine map:\n",
      " [[-1.69690669  2.33362122 -2.61461271]\n",
      " [-1.26940596  1.42219875 -2.00695106]\n",
      " [-0.79445137  1.42375524 -1.06462945]]\n",
      "\n",
      "Max absolute difference between the two outputs: 6.661e-16\n"
     ]
    }
   ],
   "source": [
    "print(\"First 3 rows of Y from stacked linear layers:\\n\", Y_stack[:3])\n",
    "print(\"\\nFirst 3 rows of Y from single affine map:\\n\", Y_eff[:3])\n",
    "print(f\"\\nMax absolute difference between the two outputs: {max_abs_diff:.3e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b823c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check (should be ~ 1e-15 to 1e-12 due to floating point)\n",
    "assert np.allclose(Y_stack, Y_eff, atol=1e-10), \"Mismatch! The equivalence failed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b498d563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conclusion: Multiple linear layers without activations are exactly one affine map.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nConclusion: Multiple linear layers without activations are exactly one affine map.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
